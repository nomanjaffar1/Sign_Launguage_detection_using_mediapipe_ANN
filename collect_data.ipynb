{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two methods for collecting data: one using MediaPipe landmarks and the other using traditional techniques. Please find the respective code for both methods below.\n",
    "\n",
    "1. To collect data using the traditional method, please use the code in the second cell.\n",
    "2. To collect data using MediaPipe landmarks, please use the code in the third cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "\n",
    "# # Main directory for storing gesture images\n",
    "# main_directory = r'C:\\Sign_Language_Detection\\gesture_data_set'\n",
    "\n",
    "# # Mapping numerical keys to gestures\n",
    "# gesture_keys = {\n",
    "#     '1': 'call',\n",
    "#     '2': 'dislike',\n",
    "#     '3': 'fist',\n",
    "#     '4': 'four',\n",
    "#     '5': 'like',\n",
    "#     '6': 'mute',\n",
    "#     '7': 'ok',\n",
    "#     '8': 'palm',\n",
    "#     '9': 'peace',\n",
    "#     '0': 'rock',\n",
    "#     '.': 'stop'\n",
    "# }\n",
    "\n",
    "# # Create main directory if it doesn't exist\n",
    "# if not os.path.exists(main_directory):\n",
    "#     os.mkdir(main_directory)\n",
    "\n",
    "# # Create subdirectories for each gesture\n",
    "# for gesture in gesture_keys.values():\n",
    "#     gesture_path = os.path.join(main_directory, gesture)\n",
    "#     if not os.path.exists(gesture_path):\n",
    "#         os.mkdir(gesture_path)\n",
    "\n",
    "# # Start capturing video from webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     _, frame = cap.read()\n",
    "\n",
    "#     # Count the number of images in each gesture directory\n",
    "#     count = {gesture: len(os.listdir(os.path.join(main_directory, gesture))) for gesture in gesture_keys.values()}\n",
    "\n",
    "#     row = frame.shape[1]\n",
    "#     col = frame.shape[0]\n",
    "#     cv2.rectangle(frame, (0, 40), (300, 300), (255, 255, 255), 2)\n",
    "#     cv2.imshow(\"data\", frame)\n",
    "#     frame = frame[40:300, 0:300]\n",
    "#     cv2.imshow(\"ROI\", frame)\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     frame = cv2.resize(frame, (48, 48))\n",
    "\n",
    "#     interrupt = cv2.waitKey(10)\n",
    "\n",
    "#     # Save images based on key press\n",
    "#     if interrupt != -1:\n",
    "#         key = chr(interrupt & 0xFF)\n",
    "#         if key in gesture_keys:\n",
    "#             gesture = gesture_keys[key]\n",
    "#             cv2.imwrite(os.path.join(main_directory, gesture, f'{count[gesture]}.jpg'), frame)\n",
    "\n",
    "#     # Break the loop on pressing 'ESC' key\n",
    "#     if interrupt & 0xFF == 27:\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Setup MediaPipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Main directory for storing gesture data\n",
    "main_directory = r'C:\\Sign_Language_Detection\\gesture_data_set'\n",
    "\n",
    "# Mapping numerical keys to gestures\n",
    "gesture_keys = {\n",
    "    '1': 'call',\n",
    "    '2': 'dislike',\n",
    "    '3': 'fist',\n",
    "    '4': 'four',\n",
    "    '5': 'like',\n",
    "    '6': 'mute',\n",
    "    '7': 'ok',\n",
    "    '8': 'palm',\n",
    "    '9': 'peace',\n",
    "    '0': 'rock',\n",
    "    '.': 'stop'\n",
    "}\n",
    "\n",
    "# Create main directory if it doesn't exist\n",
    "if not os.path.exists(main_directory):\n",
    "    os.mkdir(main_directory)\n",
    "\n",
    "# Create subdirectories for each gesture\n",
    "for gesture in gesture_keys.values():\n",
    "    gesture_path = os.path.join(main_directory, gesture)\n",
    "    if not os.path.exists(gesture_path):\n",
    "        os.mkdir(gesture_path)\n",
    "\n",
    "# Start capturing video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display\n",
    "        image = cv2.flip(image, 1)\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "        \n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "        interrupt = cv2.waitKey(10)\n",
    "        \n",
    "        # Save landmarks data based on key press\n",
    "        if interrupt != -1:\n",
    "            key = chr(interrupt & 0xFF)\n",
    "            if key in gesture_keys and results.multi_hand_landmarks:\n",
    "                gesture = gesture_keys[key]\n",
    "                gesture_path = os.path.join(main_directory, gesture)\n",
    "                count = len(os.listdir(gesture_path))\n",
    "                \n",
    "                # Save landmarks data\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    landmarks = []\n",
    "                    for landmark in hand_landmarks.landmark:\n",
    "                        landmarks.append([landmark.x, landmark.y, landmark.z])\n",
    "                    landmarks = np.array(landmarks)\n",
    "                    np.save(os.path.join(gesture_path, f'{count}.npy'), landmarks)\n",
    "\n",
    "        # Break the loop on pressing 'ESC' key\n",
    "        if interrupt & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA device count: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA device count:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cuda_device_count)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Get the current CUDA device\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m current_device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA current device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_device)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get the name of the current CUDA device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:778\u001b[0m, in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[1;32mc:\\Users\\noman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA current device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
